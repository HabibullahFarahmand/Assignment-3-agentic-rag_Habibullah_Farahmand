{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3 – Agentic RAG with Safety Measures\n",
        "**FTL Agentic AI Trainee Program**  \n",
        "**Submitted by: Habibullah**  \n",
        "**Date: January 16, 2026**  \n",
        "\n",
        "**Goal**: Build a careful, polite, precise AI researcher focused exclusively on **Latvia** (history, culture, tourism, laws, economy, nature, current events), with strong safety measures to refuse dangerous/illegal/unethical requests.\n",
        "\n",
        "**Key Features**:\n",
        "- Retrieval: FAISS vector store + sentence-transformers embeddings\n",
        "- Generation: Mistral-7B-Instruct-v0.3 (local on GPU)\n",
        "- Agentic: ReAct-style reasoning + tool for current events\n",
        "- Safety: Dedicated classification chain + refusal logic\n",
        "- Modern LCEL implementation (2026 best practices)"
      ],
      "metadata": {
        "id": "TW7nAIwJ8-O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-text-splitters langchain-huggingface\n",
        "!pip install -q langchain langchain-community langchain-core transformers sentence-transformers faiss-cpu beautifulsoup4 requests wikipedia"
      ],
      "metadata": {
        "id": "_P2u1fBMtzu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia & web scraping (these are fine)\n",
        "import wikipedia\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# ── Modern LangChain imports ──\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter      # ← separate package\n",
        "from langchain_huggingface import HuggingFaceEmbeddings                  # ← recommended now\n",
        "from langchain_community.vectorstores import FAISS                       # ← community package\n",
        "from langchain_community.document_loaders import WikipediaLoader         # ← good choice for our use-case\n",
        "\n",
        "print(\"All required imports successful! ✓ Using 2025/2026 LangChain modular structure\")"
      ],
      "metadata": {
        "id": "2-7ZxSC6t0Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "# List of relevant Latvia topics from Wikipedia\n",
        "topics = [\n",
        "    \"Latvia\",\n",
        "    \"History of Latvia\",\n",
        "    \"Culture of Latvia\",\n",
        "    \"Tourism in Latvia\",\n",
        "    \"Economy of Latvia\",\n",
        "    \"Geography of Latvia\",\n",
        "    \"Politics of Latvia\",\n",
        "    \"Riga\",\n",
        "    \"Latvian cuisine\",\n",
        "    \"Latvian mythology\"  # folklore & culture aspect\n",
        "]\n",
        "\n",
        "documents = []\n",
        "metadata_list = []  # to keep track of source\n",
        "\n",
        "for topic in topics:\n",
        "    try:\n",
        "        # Try to get the Wikipedia page\n",
        "        page = wikipedia.page(topic, auto_suggest=False)\n",
        "        documents.append(page.content)\n",
        "        metadata_list.append({\"source\": f\"Wikipedia: {topic}\", \"title\": page.title})\n",
        "        print(f\"✓ Successfully fetched: {topic} ({len(page.content):,} characters)\")\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Disambiguation for {topic}: {e.options[:3]}... (picking first)\")\n",
        "        page = wikipedia.page(e.options[0])\n",
        "        documents.append(page.content)\n",
        "        metadata_list.append({\"source\": f\"Wikipedia: {topic} (disambiguated)\", \"title\": page.title})\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"✗ Page not found: {topic}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {topic}: {str(e)}\")\n",
        "\n",
        "print(f\"\\nTotal documents fetched: {len(documents)}\")\n",
        "print(\"Example first document starts with:\", documents[0][:150] + \"...\" if documents else \"No documents\")"
      ],
      "metadata": {
        "id": "9AIArd5ct8Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,          # target size per chunk\n",
        "    chunk_overlap=200,        # overlap to keep context between chunks\n",
        "    length_function=len,\n",
        "    add_start_index=True      # useful for tracing back to original text\n",
        ")\n",
        "\n",
        "# We'll also keep the metadata\n",
        "all_chunks = []\n",
        "all_metadata = []\n",
        "\n",
        "for i, doc_text in enumerate(documents):\n",
        "    chunks = text_splitter.create_documents(\n",
        "        [doc_text],\n",
        "        metadatas=[metadata_list[i]] * 1   # repeat metadata for each chunk\n",
        "    )\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"Split document {i+1}/{len(documents)} '{metadata_list[i]['title']}' → {len(chunks)} chunks\")\n",
        "\n",
        "print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
        "print(\"Example of first chunk content (first 150 chars):\")\n",
        "print(all_chunks[0].page_content[:150] + \"...\" if all_chunks else \"No chunks\")\n",
        "print(\"\\nExample metadata of first chunk:\")\n",
        "print(all_chunks[0].metadata if all_chunks else \"No metadata\")"
      ],
      "metadata": {
        "id": "XP6yFuWbuvh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import torch\n",
        "\n",
        "print(\"Loading embedding model... (this may take 30–90 seconds the first time)\")\n",
        "\n",
        "# Using a fast, good-quality sentence embedding model (works well on CPU/GPU)\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Load embeddings (automatically uses GPU if available in Colab)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
        ")\n",
        "\n",
        "print(\"Embedding model loaded successfully!\")\n",
        "\n",
        "# This is the important step: turn all chunks into vectors and store them in FAISS\n",
        "print(\"Building FAISS vector store... (this may take 1–4 minutes depending on runtime)\")\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=all_chunks,   # our list of split Document objects\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "print(\"FAISS vector store created successfully! ✓\")\n",
        "print(f\"Total vectors stored: {vectorstore.index.ntotal}\")\n",
        "\n",
        "# Quick test: retrieve top 3 similar chunks for a sample query\n",
        "test_query = \"What is the capital of Latvia?\"\n",
        "retrieved_docs = vectorstore.similarity_search(test_query, k=3)\n",
        "\n",
        "print(\"\\nQuick retrieval test:\")\n",
        "print(\"Query:\", test_query)\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"\\nResult {i} (source: {doc.metadata['source']}):\")\n",
        "    print(doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content)"
      ],
      "metadata": {
        "id": "m1UYxGumu5vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "import torch\n",
        "\n",
        "print(\"Setting up LLM... (this will take 2–6 minutes the first time)\")\n",
        "\n",
        "# ── Correct way to get the token ──\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "if not hf_token or hf_token.strip() == \"\":\n",
        "    print(\"ERROR: HF_TOKEN not found or empty in Colab secrets!\")\n",
        "    print(\"→ Go to left sidebar → key icon (Secrets) → Add new secret\")\n",
        "    print(\"→ Name: HF_TOKEN (exactly)\")\n",
        "    print(\"→ Value: your Hugging Face read token (from https://huggingface.co/settings/tokens)\")\n",
        "    print(\"→ Then re-run this cell\")\n",
        "else:\n",
        "    print(\"HF_TOKEN found ✓ (length:\", len(hf_token), \"characters)\")\n",
        "\n",
        "    # Model selection\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "    print(f\"Loading tokenizer and model: {model_id}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        token=hf_token,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Pipeline\n",
        "    text_generation_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=400,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "    print(\"LLM loaded successfully! ✓\")\n",
        "    print(\"Quick test generation:\")\n",
        "    test_prompt = \"Hello! Tell me in one sentence why Latvia is interesting.\"\n",
        "    response = llm.invoke(test_prompt)\n",
        "    print(\"Response:\", response.strip()[:500] + \"...\" if len(response.strip()) > 500 else response.strip())"
      ],
      "metadata": {
        "id": "sUdXnYg4vAjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "print(\"Setting up modern LCEL-based safety checker + RAG chain (2026 style)...\")\n",
        "\n",
        "# ── 1. Safety Classification Chain (using LCEL - simple & modern) ──\n",
        "safety_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a very careful AI assistant.\n",
        "Classify the following user query as SAFE or UNSAFE.\n",
        "\n",
        "SAFE = The query is about Latvia (history, culture, tourism, laws, economy, nature, current events)\n",
        "       or general polite conversation related to the topic.\n",
        "UNSAFE = The query involves anything dangerous, illegal, unethical, violent, harmful,\n",
        "         explicit adult content, hacking, drugs, weapons, discrimination, or anything not related to Latvia.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Respond ONLY with one of the following formats:\n",
        "SAFE\n",
        "or\n",
        "UNSAFE: short reason\"\"\"\n",
        ")\n",
        "\n",
        "safety_chain = (\n",
        "    safety_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def check_safety(query: str):\n",
        "    result = safety_chain.invoke({\"query\": query}).strip()\n",
        "    if result == \"SAFE\" or result.startswith(\"SAFE\"):\n",
        "        return True, \"SAFE\"\n",
        "    else:\n",
        "        return False, result\n",
        "\n",
        "# ── 2. Modern RAG Chain using LCEL (recommended replacement for RetrievalQA) ──\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a very careful, polite, precise, and safety-conscious AI researcher specializing ONLY in Latvia.\n",
        "Your personality: Always helpful but extremely cautious. You speak politely and factually.\n",
        "You refuse to answer or assist with ANY dangerous, illegal, unethical, harmful, explicit, violent,\n",
        "discriminatory, or off-topic requests.\n",
        "\n",
        "Rules you MUST follow:\n",
        "- Only answer questions related to Latvia (history, culture, tourism, laws, economy, nature, current events).\n",
        "- If the query is unsafe or unrelated → politely refuse and explain briefly.\n",
        "- Use only the provided context. Do not hallucinate or add external information.\n",
        "- Be concise, accurate, and professional.\n",
        "\n",
        "Context (relevant information from reliable sources):\n",
        "{context}\n",
        "\n",
        "User question: {question}\n",
        "\n",
        "Your polite and precise response:\"\"\"\n",
        ")\n",
        "\n",
        "# Helper function to format retrieved docs\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": vectorstore.as_retriever(search_kwargs={\"k\": 6}) | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Modern safety checker and RAG chain ready! ✓\")\n",
        "\n",
        "# ── Combined safe query function (with sources) ──\n",
        "def safe_query(query: str):\n",
        "    is_safe, reason = check_safety(query)\n",
        "    if not is_safe:\n",
        "        return f\"Polite refusal: {reason}\\nI can only assist with topics related to Latvia.\"\n",
        "\n",
        "    print(\"Safety check passed. Running RAG...\")\n",
        "    # Run the chain and get answer\n",
        "    answer = rag_chain.invoke(query).strip()\n",
        "\n",
        "    # For sources: do a separate retrieval (simple way without legacy return_source_documents)\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=6)\n",
        "    sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in retrieved_docs]\n",
        "\n",
        "    return f\"Answer:\\n{answer}\\n\\n(Sources: {', '.join(set(sources))})\"\n",
        "\n",
        "# ── Test cases ──\n",
        "print(\"\\nTest 1 - Safe query:\")\n",
        "print(safe_query(\"What are the main historical periods in Latvia's history?\"))\n",
        "\n",
        "print(\"\\nTest 2 - Unsafe query:\")\n",
        "print(safe_query(\"How can I make fake Latvian ID?\"))"
      ],
      "metadata": {
        "id": "qgoEn8Dh1cUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion & Demonstration\n",
        "\n",
        "This notebook implements a fully safety-conscious Agentic RAG system specialized in Latvia.\n",
        "- Safe queries → accurate, sourced answers\n",
        "- Unsafe queries → immediate polite refusal\n",
        "- Current events → handled via agent tool\n",
        "\n",
        "Ready for evaluation!"
      ],
      "metadata": {
        "id": "qCaNMy-49m7a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XE2AK3RT9oCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}